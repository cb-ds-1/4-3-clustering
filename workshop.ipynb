{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. k-means on digits\n",
    "\n",
    "Use K-means clustering on the MNIST dataset (which can be imported with `from sklearn.datasets import load_digits`) to learn to identify the 10 digits in an unsupervised manner.\n",
    "\n",
    "1. Plot the `cluster_centers_` from a kmeans clustering (which should be shape `(10,64)`) onto a `8x8` grid (hint: use `kmeans.cluster_centers_.reshape`)\n",
    "\n",
    "2. Reorganize the clusters so that each of your learned clusters labels overlap with the true label (eg. if the digit 4 has the cluster label 9, you want to remap it to 4). Hint: You can use the statistical mode of your clusters and the real label to match them\n",
    "\n",
    "3. Now that your cluster labels are matched to the supervised learning labels, check the classification accuracy of your clustering model (accuracy score and confusion matrix). Give a two paragraph interpretation.\n",
    "\n",
    "4. Use the Fowlkes-Mallows score to evaluate your clustering against the true labels. Give a two paragraph interpretation comparing to your result in #3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "digits = load_digits()\n",
    "# Split data into 50% train and 50% test subsets\n",
    "# X_train, X_test, y_train, y_test = train_test_split(\n",
    "#     digits.data, digits.target, test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Credit: https://jakevdp.github.io/PythonDataScienceHandbook/05.11-k-means.html\n",
    "\"\"\"\n",
    "kmeans = KMeans(n_clusters=10, random_state=0)\n",
    "predicted = kmeans.fit_predict(digits.data)\n",
    "kmeans.cluster_centers_.shape\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
    "centers = kmeans.cluster_centers_.reshape(10, 8, 8)\n",
    "for axi, center in zip(ax.flat, centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "from sklearn.metrics import accuracy_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 8, 8, ..., 8, 9, 9], dtype=int32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = np.zeros_like(predicted)\n",
    "for i in range(10):\n",
    "    mask = (predicted == i)\n",
    "    labels[mask] = mode(digits.target[mask])[0]\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 3, 5, 9, 7, 0, 1, 8, 2, 6]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We get a list of the labels\n",
    "labels_un = [mode(digits.target[(predicted == i)])[0][0] for i in range(10)]\n",
    "labels_un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x216 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We zip the labels list with the list of centers\n",
    "# Then we sort this list of tuples\n",
    "zipped = zip(labels_un, centers)\n",
    "zipped = sorted(zipped, key=lambda x: x[0])\n",
    "sorted_centers = [center[1] for center in zipped]\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(2, 5, figsize=(8, 3))\n",
    "for axi, center in zip(ax.flat, sorted_centers):\n",
    "    axi.set(xticks=[], yticks=[])\n",
    "    axi.imshow(center, interpolation='nearest', cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7935447968836951"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(digits.target, labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "mat = confusion_matrix(digits.target, labels)\n",
    "sns.heatmap(mat.T, square=True, annot=True, fmt='d', cbar=False,\n",
    "            xticklabels=digits.target_names,\n",
    "            yticklabels=digits.target_names)\n",
    "plt.xlabel('true label')\n",
    "plt.ylabel('predicted label');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Totals</th>\n",
       "      <th>Misses</th>\n",
       "      <th>Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>0.994382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>127</td>\n",
       "      <td>0.302198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>177</td>\n",
       "      <td>29</td>\n",
       "      <td>0.836158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>183</td>\n",
       "      <td>29</td>\n",
       "      <td>0.841530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>181</td>\n",
       "      <td>18</td>\n",
       "      <td>0.900552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>182</td>\n",
       "      <td>46</td>\n",
       "      <td>0.747253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>181</td>\n",
       "      <td>4</td>\n",
       "      <td>0.977901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>179</td>\n",
       "      <td>2</td>\n",
       "      <td>0.988827</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "      <td>0.574713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>180</td>\n",
       "      <td>41</td>\n",
       "      <td>0.772222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Totals  Misses  Accuracies\n",
       "0      0     178       1    0.994382\n",
       "1      1     182     127    0.302198\n",
       "2      2     177      29    0.836158\n",
       "3      3     183      29    0.841530\n",
       "4      4     181      18    0.900552\n",
       "5      5     182      46    0.747253\n",
       "6      6     181       4    0.977901\n",
       "7      7     179       2    0.988827\n",
       "8      8     174      74    0.574713\n",
       "9      9     180      41    0.772222"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "We check the accuracy % of each predicted label, meaning\n",
    "how many counts exist for a given label, in a cell that isn't\n",
    "in the diagonal.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "labels = []\n",
    "totals = []\n",
    "misses = []\n",
    "accuracies = []\n",
    "for label in range(len(mat)):\n",
    "    l = list(mat[label])\n",
    "    total =  sum(l)\n",
    "    l.pop(label)\n",
    "    missed = sum(l)\n",
    "    accur = (total - missed) / total\n",
    "    labels.append(label)\n",
    "    totals.append(total)\n",
    "    misses.append(missed)\n",
    "    accuracies.append(accur)\n",
    "    \n",
    "predicts = pd.DataFrame({\n",
    "    \"Label\": labels,\n",
    "    \"Totals\": totals,\n",
    "    \"Misses\": misses,\n",
    "    \"Accuracies\": accuracies\n",
    "})\n",
    "predicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Totals</th>\n",
       "      <th>Misses</th>\n",
       "      <th>Accuracies</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>182</td>\n",
       "      <td>127</td>\n",
       "      <td>0.302198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>174</td>\n",
       "      <td>74</td>\n",
       "      <td>0.574713</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Label  Totals  Misses  Accuracies\n",
       "1      1     182     127    0.302198\n",
       "8      8     174      74    0.574713"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts.nsmallest(2, 'Accuracies')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our accuracy score is 0.79, or ~80%. This means we've accurately labeled 8 out of 10 clusters in an unsupervised way.\n",
    "\n",
    "Additionally, we have the following labels that have been mis-predicted more than 20 times: 1, 5 and 8.\n",
    "\n",
    "This means that 1's were mis-labeled 7 times out of 10, and 5's were mis-labeld nearly 4 times out of 10.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.702734687012768"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "\n",
    "fowlkes_mallows_score(digits.target, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the FM score is 0.09 percentage points lower than the accuracy_score (AS). As per the sci_kit documentation, we know that that accurary_score is an implementation of the Jaccard Index (JI). The JI, is the quotient of the intersection of two sets, over the resulting set of the same two set's union.\n",
    "Essentially, which percentage of two sets overlap, and in our case, which percentage of predicted labels match the true labels.\n",
    "\n",
    "The FM score on the other hand is a representation of the same overlap as the AS, though interpreted by quantifying the gap between the mean centers of the clusters of the predicted labels from the real label values in a geometric context. \n",
    "\n",
    "As in, quantifying the gap between the true and predicted values as a \"mean\" or average distance between two points or difference of area. It does this by calculating the ratio of the true/ideal results as a distance/value, over the percieved distance/value that seperates the distance/value of the cluster. \n",
    "\n",
    "It's formula, according to scikit, is FMI = TP / sqrt((TP + FP) * (TP + FN)). \n",
    "\n",
    "OR\n",
    "\n",
    "FMI = TP / sqrt((X) * (Y)); X = TP + FP; Y = TP + FN\n",
    "\n",
    "Where X and Y are two orthogonal vectors, each along the x and y axes respectively, and the square root of their products, as per Pythagoras, is the quantified distance between the end of these two vectors of similar or differing length. Therefore, one could summarize the FMI as being a quantifying function of how two clusters (the mean centers of the clusters being the ends of the vectors X and Y) are **different**, by gaging the distance that seperates the mean center of these two clusters in 2D space.\n",
    "\n",
    "\n",
    "Therefore, the difference between AS and FM can be highlighted as being that AS is a single-dimension comparison, on sets or lists, ie, is a value in both sets, or just one.\n",
    "Whereas the FM score is meant to evaluate on a potentially multi-dimensional context, ie, how much overlap is there between the real and predicted cluster, and in which potential direction is this difference in space so that a potential correction could be made.\n",
    "\n",
    "This is why we are seeing a difference between our AS, and our FMI. Our AS is simply telling us that our prediction was 80% accurate, in that there is a 80% overlap in values between our real and predicted sets. Whereas our FM score is telling us that in fact, our mean prediction is off by 100 - 70 = 30% on average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 2: Image Compression second pass\n",
    "\n",
    "Use any the clustering algorithms we see to produce what you think is the best image compression ratio that is **visually indistinguishable** from the original image (recall in the lecture the result was very compressed but also much uglier)\n",
    "\n",
    "Justify your choice and process in 3 paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_sample_image\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "china = load_sample_image(\"china.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(427, 640, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "china.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I haven't had time to implement this into actual code.\n",
    "\n",
    "Since we're attempting compression that optimizes for image quality and image size, we're looking at a simili-lossless compression. This would mean having as many clusters as possible, in fact, maximizing K -> {# of colours in the image}, while our resulting K < {# of colours in the image} (otherwise, it's not an actual compression.\n",
    "\n",
    "Algorithmically, this would involve running a bisection-like iteration over a starting K = 0 value, and increasing our K value until we reach a certain accuracy score, or until K = {# of colours in the image} - 1 (minimum level of compression possible). In the field of clustering, this would mean finding our\n",
    "\n",
    "Therefore, as there can be 16581375 possible combinations of RGB colors, we can afford up to 16481374 clusters. This is still a relatively large number, and we can assume the computation time for this would be quite significant as the value of K increases. This isn't ideal either should we decide to not know what kind of image we would compress. Ideally, this would mean using a Hierarchical k means clustering method.\n",
    "\n",
    "The advantage with this approach is that we're able to split down the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Clustering on Colleges\n",
    "\n",
    "This question will use the data from `college.csv`\n",
    "\n",
    "1. Use clustering with `k=2` on all data except whether the college is private of not. Try to match it best to the private/public college split (hint: don't forget to normalize your features!). Evaluate it in terms of classification metrics and clustering metrics.\n",
    "\n",
    "2. Knowing what you found in `3.1`, find the best overall clustering with the k of your choice. Make an argument and a visualization of the clusters (with two relevant statistics) to demonstrate why you think your clustering is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/college.csv')\n",
    "data.head()\n",
    "data = data.drop('Unnamed: 0',axis=1)\n",
    "columns = list(data.columns)\n",
    "columns.remove('Private')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# we basically normalize all values between 0 and 1.\n",
    "\n",
    "def normalize(minv, maxv, v):\n",
    "    return (v - minv) / (maxv - minv)\n",
    "\n",
    "for column in columns:\n",
    "    minc = data[column].min()\n",
    "    maxc = data[column].max()\n",
    "    data[column] = data[column].apply(lambda x: normalize(minc, maxc, x))\n",
    "\n",
    "# Credit: https://towardsdatascience.com/k-means-clustering-of-university-data-9e8491068778\n",
    "km = KMeans(n_clusters=2)\n",
    "fit = km.fit(data.drop('Private', axis=1))\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(n_clusters=2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "km = KMeans(n_clusters=2)\n",
    "fit = km.fit(data.drop('Private', axis=1))\n",
    "fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.08632151 0.09749031 0.13836274 0.44044701 0.70544182 0.12525339\n",
      "  0.02649792 0.5981359  0.51275933 0.20924444 0.14167259 0.80796683\n",
      "  0.87671233 0.26013625 0.48212757 0.1910198  0.62160705]\n",
      " [0.04548278 0.06008388 0.10444173 0.18270212 0.39913901 0.10567128\n",
      "  0.04672759 0.31022472 0.34219459 0.19770476 0.18146407 0.60397179\n",
      "  0.64636462 0.34116802 0.27905928 0.08051941 0.44849179]]\n"
     ]
    }
   ],
   "source": [
    "print(fit.cluster_centers_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convertToCluster(cluster):\n",
    "    if cluster=='Yes':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['Cluster'] = data['Private'].apply(convertToCluster)\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.20      0.17       212\n",
      "           1       0.65      0.56      0.60       565\n",
      "\n",
      "    accuracy                           0.46       777\n",
      "   macro avg       0.40      0.38      0.38       777\n",
      "weighted avg       0.51      0.46      0.48       777\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report\n",
    "confM = confusion_matrix(data['Cluster'],km.labels_)\n",
    "rep = classification_report(data['Cluster'],km.labels_)\n",
    "print(rep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that our clustering was relatively accurate, 72%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.24.1\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain, combinations\n",
    "import sklearn\n",
    "\n",
    "print(sklearn.__version__)\n",
    "\n",
    "def clusters_by_k(data, k=2):\n",
    "    df_o = {\n",
    "        'AtIndex': [],\n",
    "        'precision': [],\n",
    "    }\n",
    "    for i in range(1, k + 1):\n",
    "        df_o['Feature {i}'.format(i=i)] = []\n",
    "    \n",
    "    fits = []\n",
    "\n",
    "    combs = list(combinations(columns, k))\n",
    "\n",
    "    dropped = data.drop('Private', axis=1)\n",
    "\n",
    "    for comb in combs:\n",
    "        entry = {}\n",
    "        km_ = KMeans(n_clusters=k)\n",
    "        entry['fitted'] = km_.fit(dropped[list(comb)])\n",
    "        cols = ['Cluster'] + list(comb)\n",
    "        entry['confM'] = confusion_matrix(data[cols]['Cluster'],km_.labels_)\n",
    "#         print(km_.labels_)\n",
    "        entry['rep'] = classification_report(data[cols]['Cluster'], km_.labels_, output_dict=True)\n",
    "        entry['precision'] = entry['rep']['weighted avg']['precision']\n",
    "#         df_o['Labels'].append(entry['columns'])\n",
    "        df_o['AtIndex'].append(len(df_o['AtIndex']))\n",
    "        df_o['precision'].append(entry['precision'])\n",
    "        for i in range(1, k + 1):\n",
    "            df_o['Feature {i}'.format(i=i)].append(list(comb)[i - 1])\n",
    "        fits.append(entry)\n",
    "    \n",
    "    fit_df = pd.DataFrame(df_o)\n",
    "    return {'df':fit_df, 'df_o': df_o}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtIndex</th>\n",
       "      <th>precision</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.290456</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Accept</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.178991</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Enroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.576122</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Top10perc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.640613</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Top25perc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.830270</td>\n",
       "      <td>Apps</td>\n",
       "      <td>F.Undergrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.806898</td>\n",
       "      <td>Apps</td>\n",
       "      <td>P.Undergrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.438977</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Outstate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.709430</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Room.Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.328265</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.760646</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.711740</td>\n",
       "      <td>Apps</td>\n",
       "      <td>PhD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.552203</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Terminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.302710</td>\n",
       "      <td>Apps</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.444059</td>\n",
       "      <td>Apps</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.583186</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.473215</td>\n",
       "      <td>Apps</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.167126</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Enroll</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.648580</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Top10perc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.635674</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Top25perc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.124027</td>\n",
       "      <td>Accept</td>\n",
       "      <td>F.Undergrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.203606</td>\n",
       "      <td>Accept</td>\n",
       "      <td>P.Undergrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.818672</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Outstate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.708848</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Room.Board</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.768324</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.766245</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.712883</td>\n",
       "      <td>Accept</td>\n",
       "      <td>PhD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.671003</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Terminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.271151</td>\n",
       "      <td>Accept</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.797521</td>\n",
       "      <td>Accept</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.402151</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>106</td>\n",
       "      <td>0.737842</td>\n",
       "      <td>Room.Board</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107</th>\n",
       "      <td>107</td>\n",
       "      <td>0.777988</td>\n",
       "      <td>Room.Board</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>108</td>\n",
       "      <td>0.717364</td>\n",
       "      <td>Books</td>\n",
       "      <td>Personal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109</th>\n",
       "      <td>109</td>\n",
       "      <td>0.525008</td>\n",
       "      <td>Books</td>\n",
       "      <td>PhD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>110</td>\n",
       "      <td>0.550008</td>\n",
       "      <td>Books</td>\n",
       "      <td>Terminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>111</td>\n",
       "      <td>0.812883</td>\n",
       "      <td>Books</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>112</th>\n",
       "      <td>112</td>\n",
       "      <td>0.444725</td>\n",
       "      <td>Books</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>113</td>\n",
       "      <td>0.533598</td>\n",
       "      <td>Books</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114</th>\n",
       "      <td>114</td>\n",
       "      <td>0.461221</td>\n",
       "      <td>Books</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>115</td>\n",
       "      <td>0.522441</td>\n",
       "      <td>Personal</td>\n",
       "      <td>PhD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>116</td>\n",
       "      <td>0.663761</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Terminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>117</td>\n",
       "      <td>0.813735</td>\n",
       "      <td>Personal</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>118</td>\n",
       "      <td>0.795629</td>\n",
       "      <td>Personal</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>119</td>\n",
       "      <td>0.743610</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>120</td>\n",
       "      <td>0.446628</td>\n",
       "      <td>Personal</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>121</td>\n",
       "      <td>0.534975</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Terminal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>122</td>\n",
       "      <td>0.525906</td>\n",
       "      <td>PhD</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>123</td>\n",
       "      <td>0.464948</td>\n",
       "      <td>PhD</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>124</td>\n",
       "      <td>0.527008</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>125</td>\n",
       "      <td>0.627761</td>\n",
       "      <td>PhD</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>126</td>\n",
       "      <td>0.559483</td>\n",
       "      <td>Terminal</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>127</td>\n",
       "      <td>0.713130</td>\n",
       "      <td>Terminal</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>128</td>\n",
       "      <td>0.566463</td>\n",
       "      <td>Terminal</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>129</td>\n",
       "      <td>0.594239</td>\n",
       "      <td>Terminal</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>130</td>\n",
       "      <td>0.796119</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>131</td>\n",
       "      <td>0.798721</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>132</td>\n",
       "      <td>0.771775</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>133</td>\n",
       "      <td>0.445153</td>\n",
       "      <td>perc.alumni</td>\n",
       "      <td>Expend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>134</td>\n",
       "      <td>0.803448</td>\n",
       "      <td>perc.alumni</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>135</td>\n",
       "      <td>0.748204</td>\n",
       "      <td>Expend</td>\n",
       "      <td>Grad.Rate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows  4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     AtIndex  precision    Feature 1    Feature 2\n",
       "0          0   0.290456         Apps       Accept\n",
       "1          1   0.178991         Apps       Enroll\n",
       "2          2   0.576122         Apps    Top10perc\n",
       "3          3   0.640613         Apps    Top25perc\n",
       "4          4   0.830270         Apps  F.Undergrad\n",
       "5          5   0.806898         Apps  P.Undergrad\n",
       "6          6   0.438977         Apps     Outstate\n",
       "7          7   0.709430         Apps   Room.Board\n",
       "8          8   0.328265         Apps        Books\n",
       "9          9   0.760646         Apps     Personal\n",
       "10        10   0.711740         Apps          PhD\n",
       "11        11   0.552203         Apps     Terminal\n",
       "12        12   0.302710         Apps    S.F.Ratio\n",
       "13        13   0.444059         Apps  perc.alumni\n",
       "14        14   0.583186         Apps       Expend\n",
       "15        15   0.473215         Apps    Grad.Rate\n",
       "16        16   0.167126       Accept       Enroll\n",
       "17        17   0.648580       Accept    Top10perc\n",
       "18        18   0.635674       Accept    Top25perc\n",
       "19        19   0.124027       Accept  F.Undergrad\n",
       "20        20   0.203606       Accept  P.Undergrad\n",
       "21        21   0.818672       Accept     Outstate\n",
       "22        22   0.708848       Accept   Room.Board\n",
       "23        23   0.768324       Accept        Books\n",
       "24        24   0.766245       Accept     Personal\n",
       "25        25   0.712883       Accept          PhD\n",
       "26        26   0.671003       Accept     Terminal\n",
       "27        27   0.271151       Accept    S.F.Ratio\n",
       "28        28   0.797521       Accept  perc.alumni\n",
       "29        29   0.402151       Accept       Expend\n",
       "..       ...        ...          ...          ...\n",
       "106      106   0.737842   Room.Board       Expend\n",
       "107      107   0.777988   Room.Board    Grad.Rate\n",
       "108      108   0.717364        Books     Personal\n",
       "109      109   0.525008        Books          PhD\n",
       "110      110   0.550008        Books     Terminal\n",
       "111      111   0.812883        Books    S.F.Ratio\n",
       "112      112   0.444725        Books  perc.alumni\n",
       "113      113   0.533598        Books       Expend\n",
       "114      114   0.461221        Books    Grad.Rate\n",
       "115      115   0.522441     Personal          PhD\n",
       "116      116   0.663761     Personal     Terminal\n",
       "117      117   0.813735     Personal    S.F.Ratio\n",
       "118      118   0.795629     Personal  perc.alumni\n",
       "119      119   0.743610     Personal       Expend\n",
       "120      120   0.446628     Personal    Grad.Rate\n",
       "121      121   0.534975          PhD     Terminal\n",
       "122      122   0.525906          PhD    S.F.Ratio\n",
       "123      123   0.464948          PhD  perc.alumni\n",
       "124      124   0.527008          PhD       Expend\n",
       "125      125   0.627761          PhD    Grad.Rate\n",
       "126      126   0.559483     Terminal    S.F.Ratio\n",
       "127      127   0.713130     Terminal  perc.alumni\n",
       "128      128   0.566463     Terminal       Expend\n",
       "129      129   0.594239     Terminal    Grad.Rate\n",
       "130      130   0.796119    S.F.Ratio  perc.alumni\n",
       "131      131   0.798721    S.F.Ratio       Expend\n",
       "132      132   0.771775    S.F.Ratio    Grad.Rate\n",
       "133      133   0.445153  perc.alumni       Expend\n",
       "134      134   0.803448  perc.alumni    Grad.Rate\n",
       "135      135   0.748204       Expend    Grad.Rate\n",
       "\n",
       "[136 rows x 4 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit_k2 = clusters_by_k(data, k=2)\n",
    "fit_k2['df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 clustering precisions at K = 2 and with two features are:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AtIndex</th>\n",
       "      <th>precision</th>\n",
       "      <th>Feature 1</th>\n",
       "      <th>Feature 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>77</td>\n",
       "      <td>0.853325</td>\n",
       "      <td>F.Undergrad</td>\n",
       "      <td>S.F.Ratio</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.836861</td>\n",
       "      <td>Enroll</td>\n",
       "      <td>Outstate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.830270</td>\n",
       "      <td>Apps</td>\n",
       "      <td>F.Undergrad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>97</td>\n",
       "      <td>0.829275</td>\n",
       "      <td>Outstate</td>\n",
       "      <td>perc.alumni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.818672</td>\n",
       "      <td>Accept</td>\n",
       "      <td>Outstate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    AtIndex  precision    Feature 1    Feature 2\n",
       "77       77   0.853325  F.Undergrad    S.F.Ratio\n",
       "35       35   0.836861       Enroll     Outstate\n",
       "4         4   0.830270         Apps  F.Undergrad\n",
       "97       97   0.829275     Outstate  perc.alumni\n",
       "21       21   0.818672       Accept     Outstate"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The top 5 clustering precisions at K = 2 and with two features are:\")\n",
    "top_5 = fit_k2['df'].nlargest(5,'precision')\n",
    "top_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
